[notes]
### 1
Good afternoon everyone,
we are ...
### 2
The faces you are seeing are not real, they are generated

How we can teach machines to generate such a realistic faces?
### 3
- Introduction, some math and concepts behing GANs
- Model definition, how can we define models involved in GANs?
- Training: how can we train such a system?
- Some possible applications
### 4
- Framework for the estimation of generative models
- Training set -> data distribution
- Generator: prior -> sample
- Discriminator: classifier: output P(real)
### 5
Citation

GANs have completely revolutionized the field

- New applications
### 6
- Framework for estimation of generative
- Training set = data distribution
- Generator: from prior distr (latent vector, noise source)
- Discriminator: classifier, real from fake
### 7
2 player min-max game
### 8
- Only discriminator: should learn how to classify real samples by maximizing the log-probability
- D = P(real)
### 9
D and G, competition between models
### 10
Discriminator: 
- Push to one its output when feeded with real samples. In other words it should learn how real samples
- Push to zero its output when feeded with fake samples. 

Adaptive loss function, helping the generator to improve.

The idea of discriminator helps us in tasks where defining a loss function is not easy, e.g. loss function for face generation.

### 11
The power of GAN is this, you do not need a loss function
### 12
Passing to the generator.
Its role is to generate samples similar to the real ones, pushing to 1 the output of D when feeded with fake samples.
In other words the generator should learn how real samples are by trying to fool the discriminator

Problem of this loss function:
Early steps of training
### 13
Non saturating loss function, Saturating part to one but at this point the discriminator is fooled, so the training can stops
### 14
The power of GANs relies on the flexibility of the model definition, we can use every model we want in the GANs framework, the only requirement is that the model is differentiable is its parameter (e.g. a neural network).
### 18
- Let's focus on GAN training.
### 22
Notice that in the performance of the Discriminator we have also the generator network. 

- Replaced the expected value with the empirical mean

- Update the discriminator parameters, notice that we take the gradient with respect to the discriminator parameters, so G is fixed
### 24
Notice that the discriminator enters in the generator update but as before it's freezed
### 26
There are two main families of GANs
### 27
We have just discussed about the Unconditional GANs, where the generator starts only from random noise

The other big family is the family of CGAN
### 28
Conditional GAN are just normal GAN where we condition both G and D with some extra information y.

The information can be:
- class label
- source image
- image caption, etc

Conditioning is just a fancy word to say we are feeding y into G and D.

Considering MNIST we can condition our generator with the class label 1, and the generator should output only real samples with the shape of a one.
### 29
- The modified GANs game with the introduction of the condition y.

### 30
Now we'll see some interesting applications of Unconditional and conditional GANs.
### 31
Face generation: PROGAN (NVIdia) Unconditional
### 32
Domain translation. The condition can be represented by a semantic image, by a grayscale image containing only edges and so on.
### 33
Semantic image synthesis. the condition is the semantic image
### 34
Another interesting application is the image super resolution.
In this case the condition is the low resolution image the target is the high resolution image
### 35
Are GANs applied also to real world cases?
YES. 
